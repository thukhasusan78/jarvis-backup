import requests
from bs4 import BeautifulSoup
import html2text
import logging
from typing import Dict, List
from google.genai import types

# á€–á€á€„á€º Class á€€á€­á€¯ á€œá€¾á€™á€ºá€¸á€á€±á€«á€ºá€™á€šá€º (á€…á€”á€…á€ºá€žá€…á€ºá€¡á€á€½á€€á€º)
from tools.base import BaseTool

logger = logging.getLogger("JARVIS_SCRAPER")

class ScraperTool(BaseTool):
    """
    Fetches a webpage and converts it to clean Markdown text.
    Optimized for LLM reading (saves tokens).
    """
    # brain.py á€‘á€²á€€ Tool á€”á€¬á€™á€Šá€ºá€¡á€á€­á€¯á€„á€ºá€¸ á€á€­á€á€­á€€á€»á€€á€» á€•á€±á€¸á€›á€•á€«á€™á€šá€º
    name = "read_page_content"
    description = "Extract and read clean text content from a specific URL. Use this to read news, articles, or documentation efficiently."

    def get_parameters(self) -> Dict[str, types.Schema]:
        """Tool á€¡á€á€½á€€á€º á€œá€­á€¯á€¡á€•á€ºá€á€²á€· parameters á€™á€»á€¬á€¸á€€á€­á€¯ á€€á€¼á€±á€Šá€¬á€á€¼á€„á€ºá€¸"""
        return {
            "url": types.Schema(
                type=types.Type.STRING, 
                description="The full URL of the website to read (e.g., https://example.com)"
            )
        }

    def get_required(self) -> List[str]:
        """á€™á€–á€¼á€…á€ºá€™á€”á€± á€‘á€Šá€·á€ºá€•á€±á€¸á€›á€™á€šá€·á€º Parameter"""
        return ["url"]

    async def execute(self, **kwargs) -> str:
        """Agent á€€ Tool á€€á€­á€¯ á€œá€¾á€™á€ºá€¸á€á€­á€¯á€„á€ºá€¸á€›á€„á€º á€á€€á€šá€º á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€™á€šá€·á€º á€”á€±á€›á€¬ (á€™á€°á€›á€„á€ºá€¸ Logic á€™á€»á€¬á€¸)"""
        url = kwargs.get("url")
        if not url:
            return "Error: No URL provided."

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        try:
            logger.info(f"ðŸŒ Scraping URL: {url}")
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status() # Check for 404/500 errors

            # 1. Parse HTML
            soup = BeautifulSoup(response.text, "html.parser")

            # 2. Remove Junk (Ads, Navigation, Scripts) - RAM Saver
            for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
                script.decompose()

            # 3. Convert to Markdown (Clean Text)
            converter = html2text.HTML2Text()
            converter.ignore_links = False
            converter.ignore_images = True # á€•á€¯á€¶á€á€½á€± á€™á€šá€°á€˜á€°á€¸ (Token á€žá€€á€ºá€žá€¬á€¡á€±á€¬á€„á€º)
            
            markdown_text = converter.handle(str(soup))
            
            # á€…á€¬á€œá€¯á€¶á€¸á€›á€±á€€á€”á€·á€ºá€žá€á€ºá€™á€šá€º (Gemini Context á€™á€•á€¼á€Šá€·á€ºá€¡á€±á€¬á€„á€º)
            if len(markdown_text) > 10000:
                return markdown_text[:10000] + "\n...(Content truncated for brevity)"
            return markdown_text

        except Exception as e:
            logger.error(f"Scraping Error: {e}")
            return f"Failed to read page: {str(e)}"